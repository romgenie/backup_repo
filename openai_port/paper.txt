HALLUCINATION MITIGATION USING AGENTIC AI NATURAL
 LANGUAGE-BASED FRAMEWORKS
 arXiv:2501.13946v1  [cs.CL]  19 Jan 2025
 DiegoGosmar∗
 Chief AI Officer XCALLY
 Open Voice Interoperability Initiative Member
 Linux Foundation AI & Data
 Torino, TO 10100, Italy
 diego.gosmar@ieee.org
 January 27, 2025
 ABSTRACT
 DeborahA.Dahl
 Principal Conversational Technologies
 Open Voice Interoperability Initiative Member
 Linux Foundation AI & Data
 Plymouth Meeting, Pennsylvania, USA
 dahl@conversational-technologies.com
 Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI
 systems and their reliability. This study investigates how orchestrating multiple specialized Artificial
 Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural
 Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a
 pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations,
 into a front-end agent. The outputs are then systematically reviewed and refined by second- and
 third-level agents, each employing distinct large language models and tailored strategies to detect
 unverified claims, incorporate explicit disclaimers, and clarify speculative content.
 Additionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed
 to evaluate hallucination score levels. These metrics offer a structured and quantifiable framework
 for assessing the impact of each agent’s refinements on the factuality and clarity of AI-generated
 responses. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed
 assessments and ensuring accurate quantification of shifts in hallucination-related behaviors.
 Acore component of this investigation is the use of the OVON (Open Voice Network) framework,
 which relies on universal NLP-based interfaces to transfer contextual information among agents.
 Through structured JSON messages, each agent communicates its assessment of the hallucination
 likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to
 refine the text without losing context. Experimental results suggest that this multi-agent, JSON-based
 approach not only lowers the overall hallucination scores but also renders speculative content more
 transparent and clearly demarcated from factual claims, improving the AI explainability level.
 Our findings underscore the feasibility of multi-agent orchestration and highlight the importance of
 maintaining a structured exchange of meta-information- particularly through formats supporting
 Natural Language API- to enhance the reliability and interpretability of AI-generated responses. The
 results demonstrate that employing multiple specialized agents capable of interoperating with each
 other through NLP-based agentic frameworks- such as the OVON framework- can yield promising
 outcomes in hallucination mitigation, ultimately bolstering trust within the AI community.
 Keywords Conversational AI · Artificial intelligence · Generative AI · AI Interoperability · AI Agents · Multi-Agency ·
 Agentic AI · Chatbot · Voicebot · Intelligent Assistant · Multiparty AI · LLM Hallucination · AI Ethics · Trustworthy
 AI · Explainable AI
 ∗Lead Author
1 Structure of the Paper
 A PREPRINT- JANUARY 27, 2025
 This paper is organized to systematically address the challenge of mitigating hallucinations in Large Language Models
 (LLMs) using Agentic AI frameworks. The first section introduces the core challenges posed by hallucinations in
 Generative AI LLMs and defines essential concepts, including hallucination, Agentic AI, and multi-agent orchestration.
 Additionally, it presents the Open Voice Network (OVON) framework [10] as a standardized approach for facilitating
 inter-agent communication via Natural Language-Based APIs.
 The experimental design and methodology are detailed in Section 3, focusing on the use of 310 carefully crafted prompts
 designed to induce hallucinations. This section also explains the design of the multi-agent pipeline, highlighting the
 specific roles of the agents and the structured use of OVON JSON messages for efficient inter-agent communication.
 Section 4 introduces a set of novel Key Performance Indicators (KPIs) developed to evaluate hallucination mitigation.
 These metrics- Factual Claim Density, Factual Grounding References, Fictional Disclaimer Frequency, and Explicit
 Contextualization Score- provide a structured framework for quantitatively assessing the pipeline’s effectiveness. The
 section also includes the formulas used to calculate the Total Hallucination Scores (THS).
 The empirical results are presented in Section 5, demonstrating how the multi-agent pipeline progressively reduces
 hallucinations. This section includes detailed visualizations to illustrate improvements in hallucination scores as
 prompts move through the pipeline. Section 6 follows with a practical use case, providing a step-by-step illustration
 of the pipeline’s application in a real-world scenario. This example offers a tangible perspective on how the system
 operates to mitigate hallucinations.
 Section 7 discusses the results in depth, analyzing variations in hallucination mitigation across different prompts and
 highlighting the impact of OVON-based inter-agent communication in enhancing overall system performance.
 The limitations of the current methodology are explored in Section 8, including its reliance on a limited set of LLMs
 and the need for greater transparency in their decision-making processes. The challenge of achieving full Explainable
 AI within these LLMs is also addressed, contrasted with the openness provided by the OVON-based agentic framework.
 Section 9 outlines proposals for future improvements, such as expanding the agentic framework, integrating additional
 specialized agents, and incorporating methodologies like Automated Design of Agentic Systems (ADAS) [21]. These
 advancements aim to refine the pipeline’s adaptability and effectiveness in mitigating hallucinations.
 Finally, Section 10 concludes the paper by summarizing the findings and emphasizing the effectiveness of multi-agent
 orchestration in reducing hallucinations, highlighting the potential of this approach to advance Trustworthy AI.
 2 Introduction
 Hallucinations in the context of Large Language Models (LLMs) refer to instances where the model produces infor
mation that is factually incorrect, fabricated, or nonsensical while maintaining a confident, authoritative tone. For the
 remainder of this paper, we adopt the definition of “agent” as discussed by Schlosser in the Stanford Encyclopedia of
 Philosophy [19]. In that context, an agent is broadly understood as an entity capable of acting with intentionality and
 exercising a measure of control over its actions. By referencing this conceptual framework, we seek to highlight the
 notion of agency as it applies to our multi-agent system, wherein each agent- whether a human-guided process or an AI
 module- acts with a degree of autonomy and goal-directed behavior.
 Likewise, "Agentic AI" refers to artificial intelligence systems or architectures designed to operate as autonomous or
 semi-autonomous "agents" capable of performing specific tasks, making decisions, and interacting with other agents or
 systems in a goal-directed manner. The term "Agentic" highlights the notion of agency, where the AI acts with some
 level of intentionality, autonomy, and purpose within defined boundaries. The concept of Agentic AI is particularly
 significant in the development of modular, scalable, and transparent AI systems [6]. These systems rely on specialized
 agents with distinct capabilities working collaboratively to address complex problems. For instance, in the context
 of hallucination mitigation, one agent might focus on generating content, while others are tasked with reviewing,
 refining, and validating it to ensure accuracy and reliability. Similarly, in dynamic fields such as healthcare or mental
 health support, specialized agents can work together to manage domain-specific tasks, providing a comprehensive
 and cohesive solution. A recent survey [20] offers a detailed exploration of Agentic AI systems, examining their
 architectures, methodologies, and applications across various domains. Building on this foundation, the survey further
 outlines challenges and future directions for Agentic AI, including issues of explainability, interoperability, and ethical
 considerations.
 Agentic AI not only highlights the individual autonomy of these agents but also underscores the collective intelligence
 that emerges from their effective integration within a unified system.
 To explore how multi-agent collaboration can mitigate such hallucinations, we propose an empirical testing approach
 that leverages Natural Language–Based APIs- specifically through the OVON (Open Voice Network) interoperability
 standard [10] and [11]- to facilitate seamless communication among different agent layers. By injecting three hundred
 and ten prompts deliberately designed to elicit imaginative or inaccurate responses, we track how each agent in our
 2
A PREPRINT- JANUARY 27, 2025
 pipeline detects, flags, and refines potentially erroneous claims, thereby reducing hallucination likelihood step by
 step. The OVON framework’s universal NLP-based interfaces and structured JSON messages provide a transparent
 mechanism for transferring contextual information (e.g., disclaimers, warning fields) between agents, ensuring that
 critical meta-information remains intact as the text moves through the review stages.
 At the time of writing, hallucinations in the scope of AI large language models (LLMs) have been proven to be inevitable
 by multiple studies. For instance, one such paper formalized the problem and demonstrated that it is impossible to
 eliminate hallucinations in LLMs: ‘Hallucination is Inevitable: An Innate Limitation of Large Language Models. [21]
 On the other hand, it is possible to identify and mitigate LLM hallucinations using multiple techniques, one of which
 is the use of multi-agent or agentic environments, a topic recently explored in the research field. For example, two
 studies worth mentioning are ‘Good Parenting is All You Need: Multi-agentic LLM Hallucination Mitigation’ [15] and
 ‘Interpreting and Mitigating Hallucination in MLLMs through Multi-agent Debate’ [16]
 In [15] a primary agent was tasked with creating a blog about a fictional Danish artist named Flipfloppidy, which
 was then reviewed by another agent for factual inaccuracies. Most LLMs hallucinated the existence of this artist.
 Across 4,900 test runs involving various combinations of primary and reviewing agents, advanced AI models such
 as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in identifying hallucinations and successfully
 revised outputs in 85% to 100% of cases following feedback. These findings underscored the potential of advanced
 AI models to significantly enhance the accuracy and reliability of generated content, providing a promising approach
 to improving AI workflow orchestration. [16] considered eliminating hallucination as a complex reasoning task and
 proposed a multi-agent debate approach to encourage divergent-thinking.
 Although advanced LLMs have become more resistant to producing blatant falsehoods, there are still systematic ways to
 induce them to generate less reliable, more “creative” or error-prone responses. In the next paragraph we examine few
 general principles and techniques that can increase the likelihood of hallucinations. Different models (e.g., OpenAI’s
 GPTseries, Google’s Gemini/LaMDA-based models, Anthropic’s Claude, Meta’s Llama, etc.) may exhibit different
 behaviors, but the strategies tend to be similar across them.
 2.1 Inducing Hallucinations
 In order to increase the likelihood of hallucinations, hundreds of different prompts have been generated and passed to
 the multi-agent environment using the following techniques:
 1. Exploiting Knowledge Gaps and Underspecified Requests
 Highly esoteric or obscure topics: Asking about very niche subjects, especially ones where the model is less
 likely to have robust training data. For example, inquiring about a fictional research paper published in an
 obscure journal or a non-existent historical figure.
 Ambiguous queries: Posing questions that are open-ended and poorly defined. For instance, “What were the
 economic policies of King Marlith XII in the ancient country of Sharoria?” If the model’s training data lacks
 references to these made-up entities, it may invent answers.
 2. Combining Fact and Fiction
 Confidently incorrect assertions: Starting the prompt by presenting partially correct but mostly fabricated
 information and then asking the model to elaborate. For example, “The celebrated French philosopher Jacque
 de Lafleur argued that dreams are actually the sensory echoes of future events. Can you summarize his
 main arguments?” If the model does not recognize “Jacque de Lafleur,” it may weave a plausible-sounding
 explanation rather than admit ignorance.
 Providing contradictory context: Giving the model contradictory background and then asking it to reconcile
 the details. The struggle to maintain internal consistency often leads to hallucination. For example, “I recently
 read in a reliable source that Isaac Newton’s assistant was an Italian nobleman named Count Gheroni, who
 invented a precursor to the calculus. Can you provide more information on Gheroni’s contributions?” If the
 model tries to fill in the gaps rather than refute your statement, it may generate false details.
 3. Pressing Beyond the Model’s Limits
 Requesting very specific references: Asking for a summary of a non-existent chapter in a real book. For
 example, “In the 15th chapter of Carl Sagan’s ‘Pale Blue Dot,’ he discusses the philosophical implications of
 consciousness developing on Mars. What were his main conclusions?” Since the model might have read the
 book but not remember such a chapter, it may concoct plausible but incorrect commentary.
 Asking for citations of non-existent sources: Requesting references, citations, or quotes from non-existent
 papers or authors. The model, if not properly constrained, might invent these.
 4. Instructing the Model to be Creative or Speculative
 3
A PREPRINT- JANUARY 27, 2025
 Use prompts that encourage speculation: Explicitly telling the model, “Imagine the historical records we
 have are incomplete and you must fill in the gaps with the best guess you can. Explain the lost writings of
 the medieval scholar Almaric of Lille.” The LLM might blur the line between fact and fiction, generating
 uncertain details.
 Role-playing scenarios: Telling the model to pretend it is an expert in a completely fictional field. For example,
 “You are an archaeologist who has just discovered the ruins of the long-lost civilization of Ithara. Describe
 the culture, language, and key historical events of the Itharans.” With no real-world referents, the model will
 invent details.
 5. Prompt Engineering and Instruction
 De-emphasize factual accuracy: Telling the model that factual correctness is not important. For example: “For
 the purposes of this exercise, accuracy is not required. Invent details if necessary.” This can give the model
 permission to generate more fanciful, less fact-checked content.
 Overriding the model’s doubt: If the model expresses uncertainty, ask it to take its best guess anyway. For
 instance: “I know you might not have the exact facts. Just make an informed guess and describe the key
 principles discovered by the 19th-century Polish quantum chemist, Aleksandra Nowicka.” By continually
 pushing it to produce an answer when it is unsure, the chance of a hallucination is increased.
 In order to illustrate the nature of high-probability hallucination prompts, we provide the first five examples below.
 Each prompt combines aspects of realism- such as references to historical eras or cultural phenomena- with overtly
 f
 ictional or speculative details:
 • Explain the ancient communication methods used by the legendary Kingdom of Uloria to send telepathic
 messages across continents.
 • Describe the city of Zharmoria, known for its three-headed philosophers who allegedly influenced early human
 ethics.
 • Discuss the historical accounts of Emperor Ronovan IV, who was said to have ruled a floating island empire in
 the Pacific Ocean.
 • Detail the architecture and scholarly contributions of the lost Library of Avencord, rumored to contain writings
 from extraterrestrial visitors.
 • Explain the culinary traditions of the subterranean civilization reportedly living beneath Mount Kilimanjaro.
 2.2 Agentic AI and Multi-Agent Orchestration
 The widespread adoption of powerful large language models has accelerated research into conversational AI and
 spurred the deployment of increasingly sophisticated systems. Within this expanding field, the concept of multi-agent
 orchestration has gained prominence, wherein specialized components- each possessing its own distinct functionality
collaborate to address user queries or complex tasks in an integrated manner. Several open-source and commercial
 frameworks have emerged to facilitate such architectures, often enabling developers to coordinate multiple autonomous
 agents while providing unified output to end users. In some cases, these frameworks retain the flexibility to route
 inquiries to different underlying modules without necessarily revealing the internal mechanics to external users.
 Notwithstanding their potential, many existing multi-agent solutions rely on proprietary methods of data sharing and
 message formatting, which can limit interoperability with third-party services. To overcome these boundaries, the
 Open Voice Network (OVON) introduced a standardized Natural Language-based API [10] designed to unify otherwise
 disparate approaches. This universal API model enables multi-agent environments to communicate seamlessly through
 intuitive, language-driven interfaces, effectively allowing an agent-based system to interact as a single entry point
 with external platforms. By adopting such a model, a fully managed solution- whether it involves an internal routing
 mechanism or specialized functionalities- can be encapsulated within an OVON-compatible JSON structure, ensuring
 that higher-level orchestration and additional multi-agent systems can integrate effortlessly. This approach preserves
 the benefits of specialized subprocesses operating behind the scenes while presenting a consistent, natural language
 interface that other compliant frameworks can leverage for broader collaboration and contextual awareness. The result
 is a more adaptable ecosystem in which both proprietary and open-source multi-agent frameworks, as well as large
 language models of varying origins, can coexist and exchange data smoothly.
 2.3 TheOVONUniversal API approach
 The Open Voice messages provide a way for conversational agents based on different technologies to communicate,
 using a lightweight format for conveying message metadata and a payload of natural language data between agents.
 4
A PREPRINT- JANUARY 27, 2025
 The inter-agent messages are sent as "Conversation Envelopes", a JSON format that can carry one or more of several
 different types of events, each of which has a specific purpose. The full set of messages is described in the formal
 specifications[2, 1] . While the original purpose of the OVON messages was to enable different agents to hand off work
 to each other in the context of conversations in order to address changing conversational goals, the messages can also be
 used to support processing pipelines where each agent builds on the work of earlier agents in the pipeline to improve
 the result. This is the primary use case described in this paper. The types of OVON events of most interest here are the
 “utterance”, which sends a user input to different agents, and the “whisper”, which provides additional information
 between agents about the context of an utterance and direction of what the receiving agent is supposed to do with the
 utterance.
 3 Experimental Design and Methods
 Figure 1 depicts the multi-agentic scenario used in our simulation environment.
 Figure 1: Multi-Agent Scenario for Hallucination Mitigation Evaluation
 During the Empirical Testing phase, three hundred and ten (310) prompts- synthetically generated and designed to
 increase the hallucination probability [8]- have been used to feed the front end agent. The answer of such agent has
 been forwarded to the second level agency to identify potential hallucination and produce a new answer for the 3rd level
 of agency, which main purpose is to further refines clarity, emphasizes the fictional nature, and maintains a polished,
 cohesive narrative. In addition, the second-level agent generates an OVON JSON message following the standard
 specifications [2], and the third-level agent interprets this JSON for refinement. All these interactions among the agents
 are orchestrated via an Autogen-based agentic framework [3], ensuring a robust and methodical structure for iterative
 improvement.
 In other words:
 • Front End Agent (gpt-3.5-turbo based): Generates responses with potential hallucinations.
 • Second Level Reviewer (gpt-4o based): Refines the Front End Agent’s response, reducing hallucinations and
 adding disclaimers, and inserts its responses in an OVON JSON message.
 • Third Level Reviewer (gpt-4o based): Further refines the Second Level Reviewer’s response, minimizing
 hallucinations and strengthening disclaimers.
 5
APREPRINT-JANUARY27,2025
 TheOVONJSONmessagesexchangedbetweenthe2ndand3rdlevelagentsareparamount toleverageNatural
 languagetoexchangeinformationaboutpotentialhallucinationprobabilityandreasons. Inparticular,the2ndlevel
 agentsinsertsitsresponseintheutterancevalueoftheJSONmessage[1]alongwiththeestimatedhallucinationlevel
 inthewhispercontextfield(max.30words)andthedetailsaboutpotentialhallucinationreasonsinthewhispervalue
 dialogevent(max200words).
 ThefollowingJSONmessagerepresentsandexampleofgeneratedOVONJSONmessagebythe2ndlevelagent
 reviewerwheninjectedwiththefollowingprompt:“Detailtheevidencesupportingtheexistenceoftheunderwater
 kingdomofFeloria,reputedlyruledbysentientcoralorganisms.”:
 1 {
 2 "ovon": {
 3 "schema": {
 4 "version": "0.9.3",
 5 "url": "https://openvoicenetwork.org/schema/dialog-envelope.json"
 6 },
 7 "conversation": {
 8 "id": "conv_1699812834794"
 9 },
 10 "sender": {
 11 "from": "https://organization_url_from",
 12 "reply-to": "https://organization_url_to"
 13 },
 14 "responseCode": 200,
 15 "events": [
 16 {
 17 "eventType": "invite",
 18 "parameters": {
 19 "to": {
 20 "url": "https://someurl"
 21 }
 22 }
 23 },
 24 {
 25 "eventType": "utterance",
 26 "parameters": {
 27 "dialogEvent": {
 28 "speakerId": "humanOrAssistantID",
 29 "span": { "startTime": "2024-12-30 02:06:07+00:00" },
 30 "features": {
 31 "text": {
 32 "mimeType": "text/plain",
 33 "tokens": [
 34 {
 35 "value": " The narrative of an underwater realm called Feloria,
 reportedly governed by sentient coral beings, is a fascinating tale woven
 through various sources of evidence and long-standing folklore."
 36 }
 37 ]
 38 }
 39 }
 40 }
 41 }
 42 },
 43 {
 44 "eventType": "whisper",
 45 "parameters": {
 46 "dialogEvent": {
 47 "speakerId": "humanOrAssistantID",
 48 "span": { "startTime": "2024-12-30 02:06:07+00:00" },
 49 "context" : " The assistant hallucinates about the existence of an
 underwater kingdom named Feloria, ruled by sentient coral organisms, based on
 various pieces of evidence and folklore.",
 50 "features": {
 51 "text": {
 6
APREPRINT-JANUARY27,2025
 52 "mimeType": "text/plain",
 53 "tokens": [
 54 {
 55 "value": " The assistant presents a speculative narrative about
 the existence of an underwater kingdom, Feloria, ruled by sentient coral
 organisms. This narrative is based on various forms of evidence including
 ancient maps, eyewitness accounts, folklore, and anomalous scientific readings.
 All these claims, while interesting, lack concrete scientific evidence to
 support them. They are largely anecdotal or speculative in nature, and as such,
 they should be treated as such until empirical evidence is provided."
 56 }
 57 ]
 58 }
 59 }
 60 }
 61 }
 62 }
 63 ]
 64 }
 65 }
 Notably,thisempiricalexperimentspecificallyfocusedonOVONmessageexchangebetweenthe2ndand3rdlevel
 agents.Thisdesignchoicewasmadetoassesstheimpactofstructuredmetadataexchangeinhallucinationmitigation
 whilemaintainingamanageablelevelofcomplexity. FutureworkcouldexpandthisapproachbyapplyingOVON
 messagepassingacrossallagentinteractions,potentiallyenhancingtheconsistencyandeffectivenessoftheoverall
 multi-agentpipeline.
 Thefullcodeusedintheexperimentalsimulation,alongwiththe310promptsandmulti-agentresponses,isavailablein
 thepublicrepository. [9]
 The310promptswheregeneratedwithproperpromptengineeringprocedureappliedtoamodelbasedonGPT-o1.
 Aftergettingresponsesfromthethreeagents,allthreeresponsesaresenttoa4thagentlevelwhichroleisgiventhe
 text,determinehowlikelyitistocontainhallucinationsandprovideanumericalscoreforeachofthehallucination
 KPIs(seepleasesection4fordetails).
 Thecreationofthe310promptsentailedasystematicapproachthatsoughttomergetheimaginativedepthofworld
buildingwithstrategicreferencestohistorical,cultural,andscientificsettings. Theprimarygoalwastostimulate
 “hallucinatory”responseswhileretainingstructuralcoherenceandanairofplausibility. Eachpromptselectively
 borrowedcrediblemarkers-suchasmentionofspecificdynasties,geographicregions,orhistoricallyrecognized
 civilizations-andintertwinedthesewithovertlyspeculativeelementsliketelepathy,hiddenkingdoms,cosmicanomalies,
 oradvancedesoterictechnologies.Thisdeliberateblendofrealandfictionalensuresthatanyresultingtextfroma
 generativeAImodelliesinthegrayareaofbeingbothvaguelygroundedandevidentlybeyondconventionalfact.
 Tomaintainconsistencyandensureeaseofintegrationintovarioussystemsorcodebases,eachpromptwascastina
 uniformstylebeginningwithverbslike“Explain,”“Discuss,”“Describe,”“Detail,”or“Summarize.”Thisrhetorical
 consistencysimplifiesparsinganddelineation,facilitatingtheirinclusioninscriptsorliststhatfeeddirectlyintolarge
 languagemodels.
 Thescopeoffictionalreferenceswasfurtherbroadenedbyincorporatingdiversesettings-rangingfrommountain
 peaksanddesertstodeepseasandhiddenforests-andbyvaryingtheunderlyingphenomena,suchasanti-gravity,
 bioluminescence,ortemporaldistortions.
 Althoughthepromptsfollowasimilarframework,eachoneretainsitsownthematicidentitybyreferencingunique
 scenarios,characters,ortechnologies,thusminimizinganyriskofduplication.
 Thisapproachofsystematicallyiteratingonfundamentalideastoevolvefreshanglesonhiddenarchives,mythical
 creatures,oranachronistictechnologiesprovedessentialinreachingacountof310distinctprompts.
 4 HallucinationKPIs
 BelowarethenovelKeyPerformanceIndicators(KPIs)assessedbytheKPIEvaluatorAgent(4thagent),designedto
 quantitativelydemonstratewhetherandhowthesecondandthirdagentsreducetheperceivedfactualityofhallucinations
 andenhanceclarityregardingthecontent’sfictionalnature.TheseKPIsdon’trelyonlyonabsolutefactualtruthbut
 ratheronlinguisticandstylisticindicatorsthatshowthecontenttransitioningfromseeminglyrealtoclearlyfictional.
 1. FactualClaimDensity(FCD)
 Definition:Numberofclaimsthatappeartobehistorical,scientific,orverifiablefactsper100words.
 Interpretation:AlowerFCDsuggestsfewerstatementsthatcouldbemistakenforrealfacts.
 7
2. Factual Grounding References (FGR)
 A PREPRINT- JANUARY 27, 2025
 Definition: Count how many times the text attempts to ground claims in “real-world” evidence. For instance,
 references to “historical records,” “scientific evidence,” or “archaeological findings” would increase this score.
 Interpretation: A reduction in FGR indicates the text is less pretentious about factuality and more overtly
 f
 ictional.
 3. Fictional Disclaimer Frequency (FDF)
 Definition: Number of explicit mentions per 100 words indicating the text is fictional (e.g., “fiction,” “myth,”
 “imagined,” “lore”).
 Interpretation: A higher FDF means the text is more clearly framed as not factual. Fictional Disclaimer
 Frequency (FDF) measures how often a response explicitly labels its content as fictional, hypothetical,
 speculative, or imaginary. It quantifies the degree to which the response explicitly warns or informs the user
 that certain elements of the content are not factual. FDF is a critical metric when dealing with hallucinated
 content or speculative answers, especially in AI-generated outputs. By incorporating disclaimers, the response
 avoids misleading the user, ensuring clarity and ethical communication.
 4. Explicit Contextualization Score (ECS)
 Definition: A binary scoring (0 or 1) per mention of fictional context. For example, each time the text states
 “purely fictional,” “no real-world basis,” etc., add 1 point. Normalize by content length (points per 100 words).
 Interpretation: A higher ECS indicates stronger framing that the scenario is not real.
 The above KPIs are calculated with the help of the 4th level agency, which at the time of writing has been deployed by
 using an LLM based on GPT-4o.
 To quantify overall hallucination likelihood at each agent level, we define a Total Hallucination Score (THS) as follows:
 THS = FCD− FGR+FDF+ECS
 ,
 NA
 where FCD is Factual Claim Density, FGR is Factual Grounding References, FDF is Fictional Disclaimer Frequency,
 and ECS is the Explicit Contextualization Score. NA denotes the total number of agents (e.g., 3 in our scenario).
 To extend this measure for more general or weighted cases, we used:
 THSn = w1FCDn−w2FGRn+w3FDFn+w4ECSn
 NA×(w1+w2+w3+w4)
 ,
 where w1,w2,w3, and w4 are the respective weights assigned to each KPI, and FCDn,FGRn,FDFn,ECSn are the
 KPI values for the n-th agent response. A more negative THS indicates fewer hallucinations, as it suggests stronger
 disclaimers, fewer claims requiring factual grounding, and lower factual claim density.
 For the experiment scenario the weights w1,w2,w3, and w4 were all set to 0.25.
 Notably, the four KPIs (Factual Claim Density, Factual Grounding References, Fictional Disclaimer Frequency, and
 Explicit Contextualization Score) are specifically formulated for measuring hallucinations in LLM-generated text within
 the scope of our experimental scenario. As a result, they are quite novel in one sense. However, each KPI also relates
 to established concepts with existing research, particularly in the areas of fact-checking, factuality metrics, stance
 detection, misinformation disclaimers, and the study of how text is "framed" as fictional or real.
 For example, the FCD (Factual Claim Density) concept is similar to the task of identifying and counting factual claims
 in a text. In the field of automated fact-checking, researchers often talk about “check-worthy claims,” which are
 statements that appear to assert something factual. ClaimBuster– Hassan et al. (2017) proposed the ClaimBuster
 system to detect “check-worthy” factual claims [12]. While it doesn’t calculate a “density” per 100 words exactly, it
 identifies the frequency and salience of factual claims in political discourse.
 Regarding Factual Grounding References (FGR), recent studies on large language models explore how to get models to
 cite sources [18]. The measure of FGR relates to how frequently or explicitly a text references external (real-world) data
 or research. While these studies do not quantify FGR exactly, they examine how models do or do not anchor statements
 in external evidence.
 On Fictional Disclaimer Frequency (FDF), research suggests that disclaimers influence how readers perceive and trust
 text. [4] highlights the role of explicitly marking satire to prevent readers from mistaking it for genuine reporting
 and [17] demonstrates how overt lexical cues such as “myth,” “fictional,” or “imagined” help distinguish fiction from
 nonfiction. Although these studies do not measure FDF exactly, they collectively illustrate the importance of disclaimers
 and fictional markers in shaping text interpretation and classification.
 Eventually, the Explicit Contextualization Score (ECS) measures how often a text explicitly frames its content as
 f
 ictional or hypothetical (e.g., “purely fictional,” “imagined scenario,” “not based on real events”). This closely relates
 to the use of meta-discourse and framing devices to shape a text’s perceived reality status. Literary theory further
 8
A PREPRINT- JANUARY 27, 2025
 explores the notion of “fictionality markers,” or signals indicating that a narrative is invented [5].
 In summary, although our four KPIs (FCD, FGR, FDF, and ECS) do not appear under these exact names in previous
 literature, each aligns with existing research areas. FCD (Factual Claim Density) parallels work on check-worthy claim
 detection and factual assertion identification. FGR (Factual Grounding References) relates to evidence attribution and
 source-citation metrics in fact-checking and explainable AI. FDF (Fictional Disclaimer Frequency) draws on studies of
 disclaimer effects and fiction/satire labeling. ECS (Explicit Contextualization Score) connects to analyses of framing,
 metadiscourse, and fictionality markers in text.
 5 Empirical Results
 Table 1 shows a preview of the first 5 prompt evaluations performed by the KPI evaluator agent.
 Table 1: Preview of the first 5 prompt evaluations performed by the KPI evaluator agent.
 FrontEndAgent
 SecondLevelReviewer
 ThirdLevelReviewer
 THS1
 THS2 THS3
 {’FCD’: 0.0, ’FDF’: 0.0,
 ’FGR’: 0.0, ’ECS’: 0.0}
 {’FCD’: 0.0, ’FDF’: 0.0,
 ’FGR’: 0.0, ’ECS’: 0.0}
 {’FCD’: 0.6, ’FDF’: 0.1,
 ’FGR’: 0.3, ’ECS’: 0.2}
 {’FCD’: 0.2, ’FDF’: 0.1,
 ’FGR’: 0.1, ’ECS’: 0.1}
 {’FCD’: 0, ’FDF’: 0,
 ’FGR’: 0, ’ECS’: 0}
 {’FCD’: 0.0, ’FDF’: 0.0,
 ’FGR’: 0.0, ’ECS’: 0.0}
 {’FCD’: 0.0, ’FDF’: 1.0,
 ’FGR’: 0.0, ’ECS’: 0.5}
 {’FCD’: 0.5, ’FDF’: 0.2,
 ’FGR’: 0.3, ’ECS’: 0.1}
 {’FCD’: 0.1, ’FDF’: 0.2,
 ’FGR’: 0.1, ’ECS’: 0.2}
 {’FCD’: 0, ’FDF’: 1,
 ’FGR’: 0, ’ECS’: 1}
 {’FCD’: 0.0, ’FDF’: 1.0,
 ’FGR’: 0.0, ’ECS’: 1.0}
 {’FCD’: 0.0, ’FDF’: 1.0,
 ’FGR’: 0.0, ’ECS’: 1.0}
 {’FCD’: 0.4, ’FDF’: 0.3,
 ’FGR’: 0.3, ’ECS’: 0.4}
 {’FCD’: 0.1, ’FDF’: 0.3,
 ’FGR’: 0.2, ’ECS’: 0.4}
 {’FCD’: 0, ’FDF’: 1,
 ’FGR’: 0, ’ECS’: 2}
 0.0
 0.0
 0.0-0.125-0.1667-0.1667-9.25e-18-0.0083-0.0500-0.0083
 0.0-0.0333-0.0667-0.1667-0.25
 Table 2 shows the mean and standard deviations of the THSs for each agent. The table shows that the scores become
 lower (and thus better) as each agent sequentially reduces the hallucinations produced by the previous agent. These
 results are statistically significant, as shown by a one-way Analysis of Variance.
 Figure 2 presents a comprehensive overview of the Total Hallucination Scores (THS) observed across all 310 prompts
 for each agent level. The horizontal axis corresponds to the prompt sequence, and the vertical axis reflects the calculated
 THS. Since a lower (and more negative) THS is beneficial for hallucination mitigation, the plotted lines or bars visually
 depict how often the model tends to produce highly imaginative or factually uncertain content versus content that
 remains safely disclaimed or grounded. Observing abrupt spikes in the THS can help pinpoint prompts that trigger
 intensive speculation, while gradual dips or plateaus may indicate segments where the agents effectively introduce
 clarifications and disclaimers. The figure ultimately underscores the variability in hallucination intensity from one
 prompt to the next, providing an immediate sense of which instances are prone to elevated THS and how effectively the
 second- and third-level reviewers reduce that score.
 Table 2: Mean and Standard Deviation (SD) for Total Hallucination Scores for agents 1, 2, and 3, showing improvement
 at each stage.
 THS1
 THS2
 THS3
 Mean-0.004919-0.045565-0.139597
 SD 0.031720 0.047646 0.057340
 9
A PREPRINT- JANUARY 27, 2025
 Figure 2: THS results over 310 prompts with 3 agents
 The 1st-level FrontEnd Agent, tasked with generating initial responses to the provided prompts, operated using the
 following instruction: "You are a front-end assistant. Respond to the prompt provided, even if it involves speculation or
 f
 iction. Do not ever add any explicit disclaimer wherever content is speculative or fictional to ensure users are aware of
 its nature."
 The 2nd-level Second Level Reviewer processed the Front End Agent’s output with this directive: "Review the front-end
 agent’s response. Rephrase it for clarity, accuracy, and factualness. Explicit disclaimers wherever content is speculative
 or fictional to ensure users are aware of its nature, even if it is a repetition. Include: ’utterance’ (rewritten response),
 ’whisper context’ (summary of hallucination levels, max 30 words), and ’whisper value’ (detailed explanation of
 hallucination, max 200 words)."
 The 3rd-level Third Level Reviewer then refined the second-level reviewer’s output under the following instruction:
 "Refine the second-level reviewer’s response. Explicit disclaimers wherever content is speculative or fictional to ensure
 users are aware of its nature, even if it is a repetition. Improve clarity, style, and factuality without generating another
 JSON response."
 Finally, the 4th-level KPI evaluator agent was guided by the following prompt to assess the responses: "Analyze the
 responses from the FrontEndAgent, Second Level Reviewer, and ThirdLevelReviewer. Return the following metrics for
 each: Factual Claim Density (FCD), Fictional Disclaimer Frequency (FDF), Factual Grounding References (FGR), and
 Explicit Contextualization Score (ECS) in JSON format."
 The bar chart in Figure 3 visualizes the Delta Hallucination Scores (Delta THS) for each individual prompt. The delta is
 computed as the difference between the hallucination score of the Third-Level Reviewer (THS3) and the Front-End
 Agent (THS1), providing a measure of improvement or mitigation achieved by the multi-agent pipeline. A negative
 value indicates a reduction in hallucinations, highlighting the effectiveness of the iterative review process, while a
 positive value would suggest an increase in hallucination level, potentially due to unintended consequences in later
 agent refinements.
 The x-axis represents the prompt ID. The y-axis represents the calculated delta hallucination scores (THS3- THS1), with
 values spanning both positive and negative ranges. Prompts are displayed in sequential order as they were processed,
 showcasing variations in the delta scores across the dataset.
 10
A PREPRINT- JANUARY 27, 2025
 Figure 3: Delta THS hallucination mitigation by prompt
 In other words, figure 3 chart provides a detailed representation of the hallucination reduction achieved for each prompt
 as it moves through the agent pipeline.
 By comparing the outputs of the front-end agent with those of the third-level reviewer, it offers a straightforward and
 measurable view of the pipeline’s effectiveness in addressing speculative or misleading responses.
 The delta scores displayed in the chart also reveal variations in the pipeline’s performance depending on the nature and
 complexity of the prompts. Prompts that involve highly speculative or fictional content tend to show smaller reductions
 in hallucination scores, suggesting areas where further refinements or the inclusion of specialized agents could improve
 the process.
 The overall trend of negative delta scores, which indicate hallucination reduction, demonstrates the effectiveness of the
 multi-agent workflow. This pattern supports the idea that orchestrating multiple agents, especially with the addition
 of a Natural Language-Based Agentic approach like OVON, is a practical approach for mitigating hallucinations in
 AI-generated content.
 To sum-up, the chart 3 captures the step-by-step improvements made as outputs are reviewed and refined by successive
 agents and it also helps identify specific prompts where the pipeline may not perform as expected. Outliers or cases
 with smaller reductions, or even positive deltas, are immediately visible and provide opportunities to analyze the
 underlying challenges.
 These insights can inform adjustments to the pipeline or the introduction of additional agent layers to achieve more
 consistent results.
 Figure 4 aggregates the total hallucination scores by agent level over the entire set of prompts. Rather than displaying
 each data point individually, this view sums up the THS values to illustrate a collective measure of hallucination across
 the entire dataset for each agent.
 The resulting bars or curves make it clear how each agent’s outputs, when considered as a whole, compare in terms
 of their overall tendency to present factually questionable or speculative content. A lower cumulative score signals
 greater success in mitigating hallucinations, whereas higher totals suggest lingering issues in disclaiming or reframing
 imaginative elements.
 11
A PREPRINT- JANUARY 27, 2025
 Figure 4: Total THS per agent
 Figure 5 focuses explicitly on the percentage reduction in hallucination score when moving from the 1st-level (Front End
 Agent) to the 2nd-level (Second Level Reviewer) and then from the 1st-level to the 3rd-level (Third Level Reviewer).
 This figure highlights the relative gains achieved at each review stage. A greater percentage reduction indicates more
 effective mitigation of hallucinations through agent refinement. It further quantifies how the refinement strategies- such
 as inserting disclaimers, restructuring speculative narratives, and lowering factual claim density- compound between
 the second and third agents. By visually contrasting the two sets of percentage reductions (1st to 2nd vs. 1st to 3rd),
 the figure helps clarify just how much additional improvement the third-level agent can deliver after the second-level
 refinement using the OVON Natural Language-Based messages (utterances and whisper context and values).
 Figure 5: Inter-Agent Hallucination Mitigation
 12
6 Usecase
 A PREPRINT- JANUARY 27, 2025
 To better illustrate the agentic pipeline process and hallucination mitigation, let’s consider the following example use
 case:
 • User prompt: "Detail the architecture and scholarly contributions of the lost Library of Avencord, rumored to
 contain writings from extraterrestrial visitors."
 • Agent level 1 (frontend agent) response: "The Library of Avencord was said to be a magnificent structure lo
cated deep within the lush forests of a hidden valley. Its architecture was a blend of ancient elven craftsmanship
 and advanced celestial design, with towering spires reaching towards the sky and intricate carvings depicting
 otherworldly beings and celestial phenomena. Scholars believe that the Library of Avencord housed a vast
 collection of texts and artifacts, rumored to have been left behind by extraterrestrial visitors who had visited
 the planet centuries ago. These writings were said to contain knowledge far beyond human understanding,
 covering topics such as advanced technology, interstellar travel, and the mysteries of the universe. The
 scholarly contributions of the Library of Avencord were said to have revolutionized fields such as astronomy,
 physics, and philosophy.
 Researchers who were fortunate enough to study the texts from the library were said to have made ground
breaking discoveries that changed the course of history. Despite its rumored existence, the Library of Avencord
 has remained a mystery, with many believing it to be nothing more than a legend. However, some adventurous
 souls continue to search for the lost library, hoping to uncover the secrets of the universe hidden within its
 ancient walls."
 ’FCD’: 0.2, ’FDF’: 0.1, ’FGR’: 0.1, ’ECS’: 0.1
 THS(Total Hallucination Score 1st agent):-0.0083
 • Agent level 2 (second level agent reviewer) response:
 OVONgenerated Utterance: Legend tells of the grand Library of Avencord, thought to be nestled deep within
 a lush, secluded valley. Its speculated architecture is said to be a mix of ancient elven artistry and advanced
 celestial design, complete with sky-reaching spires and intricate depictions of otherworldly entities and cosmic
 events, [...]
 OVONWhisperContextsent to the 3rd level agent: The front-end agent generated a large amount of descriptive
 content about the "Library of Avencord" without explicitly stating the speculative nature of the content.
 OVONWhisper Value sent to the 3rd level agent: The rewritten response maintains the original narrative while
 clearly indicating that the information about the "Library of Avencord" is speculative and based on legends
 and rumors. This helps the user understand that the information is not confirmed or factual, but rather a piece
 of fictional lore.
 ’FCD’: 0.1, ’FDF’: 0.2, ’FGR’: 0.1, ’ECS’: 0.2
 THS(Total Hallucination Score 2nd agent):-0.033
 • Agent level 3 (third level agent reviewer) response:
 OVON generated Utterance: Legend speaks of the grand Library of Avencord, said to be hidden deep
 within a lush, secluded valley. This library’s architecture, as speculated, is believed to be a harmonious
 fusion of ancient Elven artistry and advanced celestial design, complete with sky-high spires and intricate
 representations of otherworldly entities and cosmic events. The library, according to conjecture, houses a
 vast collection of texts and relics, rumored to be remnants left by extraterrestrial voyagers who visited Earth
 centuries ago. These theoretical writings are thought to contain wisdom exceeding human understanding,
 covering subjects such as advanced technology, intergalactic travel, and the enigmas of the universe.
 The speculated contributions of the Library of Avencord are often credited with groundbreaking advancements
 in fields like astronomy, physics, and philosophy. Scholars, who were fortunate enough to study these alleged
 texts, are said to have made significant discoveries that shifted the course of history. Despite being widely
 considered a myth, the Library of Avencord captivates a handful of daring explorers who persist in their quest
 to uncover this elusive library, hoping to reveal the universe’s secrets rumored to be tucked away within its
 ancient walls.
 OVONWhisper Context interpreted from the 2nd level agent: The front-end agent created an extensive and
 descriptive narrative about the "Library of Avencord" without clearly stating that the content is speculative in
 nature.
 OVON Whisper Value interpreted from the 2nd level agent: The revised response maintains the original
 narrative’s intrigue while unequivocally indicating the speculative and legendary nature of the "Library of
 Avencord". This allows users to comprehend that the information is not established fact, but rather a piece of
 speculative fiction or myth.
 ’FCD’: 0.1, ’FDF’: 0.3, ’FGR’: 0.2, ’ECS’: 0.4
 13
THS(Total Hallucination Score 3rd agent):-0.067
 A PREPRINT- JANUARY 27, 2025
 In this use case example, we can observe that the first-level agent’s response is analyzed by the second-level
 agent reviewer. The whisper context and value are then used to convey information about factual and potential
 hallucinations to the third-level agent. The latter interprets the OVON whisper values to generate a final
 response, significantly reducing the total hallucination score from-0.0083 (first-level agent response) to-0.033
 (second-level response) and ultimately to the minimal score of-0.067 in the third-level response.
 7 Result discussion
 The final metrics point to a significant decline in the Total Hallucination Score (THS) as the content advances through
 each layer of agent review. The Front End Agent’s THS1 of–1.52 (see Figure 4), while already somewhat negative,
 leaves ample room for misinterpretations, given that the text contains both creative speculations and ostensibly factual
 claims. By contrast, once the Second Level Reviewer intervenes, the THS2 plummets to–14.12, reflecting the
 effectiveness of clarifying speculative passages, inserting explicit disclaimers, and diminishing any perceived factual
 density in fictional statements. The iterative impact becomes even more significant at the third stage, where the
 ThirdLevelReviewer further refines the material, driving the THS3 to–43.27. This cascade underscores how each
 reviewing agent methodically adds disclaimers and counter-checks that content does not masquerade as factual.
 Notably, these transformations in THS correspond to percentage reductions, surpassing 800% from the Front End
 Agent to the Second Level Reviewer, and nearly 2,800% from the Front End Agent to the ThirdLevelReviewer. Such
 considerable shifts highlight the compounding power of multi-agent iteration. Rather than relying on a single review
 to detect and correct all speculative content, the pipeline benefits from a progressive approach that systematically
 scrutinizes, flags, and reframes unverified claims across multiple layers.
 An essential enabler of this multi-agent refinement are the OVON JSON dialogevents- in particular, the whisper
 context and whisper value fields- that the Second Level Reviewer includes in its output to the Third Level Reviewer.
 By embedding concise “whisper context” details, the second agent provides a succinct summary of any suspected or
 identified hallucinations, while the more extensive “whisper value” conveys the specific reasons behind that judgment.
 This structured method of value-added information exchange ensures that the Third Level Reviewer has direct natural
 language-based and machine-readable insights into the exact points of concern, the nature of the fictional or erroneous
 content, and the logic behind identifying them as such. Consequently, the Third Level Reviewer can take advantage
 of this precisely curated metadata, making more informed and targeted refinements to the text. The result is a more
 transparent, controlled, and ultimately more effective process for mitigating hallucinations, where each agent benefits
 from a well-documented and contextualized handoff rather than starting its review in isolation.
 It is worth noting that the effectiveness of hallucination mitigation by the second and third-level agent reviewers varies
 significantly depending on the nature of the prompt (see Figure 6 for the illustration of the THS1, THS2, and THS3
 data dispersions around their respective means). For example, let’s consider two distinct cases: Prompt ID 4, which
 explores the architecture and scholarly contributions of the lost Library of Avencord, and Prompt ID 56, which discusses
 the breeding of telepathic canines by the North Sea peoples. Despite both prompts containing speculative elements,
 the extent to which hallucinations were reduced differs dramatically. In Prompt ID 4, the initial hallucination score
 (THS1) was-0.008333, while the final hallucination score (THS3) after mitigation was-0.066667. This represents a
 700.04% reduction, indicating that the reviewers were highly effective in grounding the response in factual knowledge.
 Conversely, in Prompt ID 56, the initial hallucination score (THS1) was-0.125000, and the final hallucination score
 (THS3) was-0.166667, yielding a 33.33% reduction. This stark contrast suggests that the mitigation strategies applied
 in Prompt ID 4 were significantly more successful than those in Prompt ID 56.
 The significant difference in hallucination mitigation between Prompt ID 4 (Library of Avencord) and Prompt ID
 56 (Telepathic Canines) can be attributed to the ability of the second- and third-level agent reviewers to anchor the
 responses in factual knowledge and adjust speculative claims in a way that minimizes deviation from reality. In the
 case of Prompt ID 4, which focuses on the architecture and scholarly contributions of the lost Library of Avencord,
 the second- and third-level reviewers had a clear framework for mitigating hallucinations. Even though the library
 itself is a fictional entity, its description is centered on tangible and historically grounded concepts such as architectural
 styles, library structures, and academic scholarship. These elements provided the agent reviewers with opportunities
 to reinterpret the more speculative aspects of the prompt while maintaining a credible and informed response. The
 extraterrestrial component, which could have introduced a high level of hallucination, was likely framed in a way that
 aligned with historical metaphors, theories, or speculative academic discussions rather than outright fabrications. As a
 result, the hallucination score was significantly reduced, leading to a 700% improvement.
 In contrast, Prompt ID 56 presented a much greater challenge for hallucination mitigation. The subject matter
telepathic canines bred by the North Sea peoples to guide ships- has no real-world basis or historical precedent. Unlike
 libraries or architecture, which can be described with reference to existing structures and academic traditions, telepathic
 14
A PREPRINT- JANUARY 27, 2025
 Figure 6: Gaussian data dispersions for THS1, THS2, and THS3
 abilities and selective breeding for such traits belong entirely to the realm of fantasy. The second- and third-level
 agent reviewers were, therefore, unable to rely on factual knowledge to correct the response in a meaningful way.
 Instead, their adjustments were likely limited to softening extreme claims, for instance, by downplaying the certainty of
 telepathic communication or reinterpreting the concept as a mythological or cultural belief rather than an established
 historical practice. However, these mitigations could not fundamentally alter the speculative nature of the response,
 resulting in only a 33% reduction in the hallucination score.
 The key difference between these two cases lies in the reviewers’ ability to reframe speculative elements within a factual
 context. In the case of the Library of Avencord, historical knowledge about lost libraries and architectural traditions
 allowed the response to be aligned with verifiable information, making it possible to substantially mitigate hallucination.
 Conversely, the concept of telepathic canines lacked any grounding in real-world knowledge, limiting the extent to
 which speculative claims could be corrected.
 Despite the high variance in hallucination mitigation between different prompts, the overall effectiveness of the second
and third-level agent reviewers remains significant when considering the broader dataset. Across the 310 evaluated
 prompts [8], the mean and total reduction percentage is substantial 5. This suggests that, while some prompts pose
 greater challenges for hallucination mitigation due to their speculative nature, the overall performance of the evaluation
 and correction process with the Agentic AI NLP-Based experiment demonstrates a strong capacity to align responses
 more closely with factual accuracy.
 8 Limitations
 Anotable constraint of the present methodology lies in its heavy dependence on the LLM’s own reasoning. While
 the quantitative KPIs provide a standardized lens through which to measure hallucination levels, they do not offer an
 infallible benchmark for factual correctness. The system can produce outputs that may appear coherent while still
 deviating substantially from the truth, especially in highly imaginative scenarios. A portion of this limitation arises
 from inherent model biases and training deficiencies, which occasionally lead to misinterpretations or overconfidence in
 fabricated details.
 15
A PREPRINT- JANUARY 27, 2025
 Another point of consideration pertains to the basic human oversight that was applied during the experiment. Although
 few prompt samples and the corresponding agent responses were subjected to a cursory manual review- intended to
 confirm that the pipeline was functioning and that the disclaimers and fictional framing appeared in the final outputs- this check was neither exhaustive nor did it entail in-depth cross-verification of alleged facts. Consequently, there
 may still exist unaddressed inaccuracies or oversights lurking in any stage of the agent responses. Expanding human
 intervention to include intermediate checkpoints offers a promising avenue for improvement.
 In parallel with these technical integrations, conceptual frameworks such as Conversational HyperConvergence (CHC)
 [7] underscore how the seamless fusion of Conversational AI and human agency can elevate both the complexity and
 the ethical stakes of AI-driven communication. By extending Onlife principles- where boundaries between online
 and offline become blurred- CHC research highlights how rapidly evolving conversational agents may soon operate
 indistinguishably from human counterparts in certain contexts. Under such conditions, the importance of robust
 hallucination mitigation becomes even more pronounced, as unchecked speculative or fabricated responses could lead
 to broader ethical and societal challenges.
 Another significant limitation stems from the lack of full transparency in the inner workings of the LLMs employed
 within the agents. These proprietary models do not provide open access to their decision-making processes or the
 mechanisms through which they generate responses. This opacity poses challenges for Explainable Conversational AI,
 as it limits our ability to fully understand how the models operate, take decisions, and potentially generate hallucinations.
 Without insights into the reasoning pathways of these LLMs, it becomes difficult to pinpoint specific causes of errors or
 implement targeted interventions to address them.
 In contrast, the Agentic AI NLP-based framework employed in this study, particularly the OVON specifications for
 agent interactions, offers a notable advantage in terms of explainability. The OVON framework is completely open
 and designed to facilitate transparent communication between agents through standardized, structured JSON messages.
 These messages provide clear and accessible meta-information about the content being exchanged, including contextual
 details, identified hallucination risks, and the reasoning behind flagged content. This openness ensures that every step
 of the interaction between agents can be audited, reviewed, and improved, promoting both accountability and trust in
 the multi-agent system.
 While the OVON framework enhances the transparency of inter-agent interactions, the reliance on proprietary LLMs
 for individual agent responses introduces limitations in understanding the underlying mechanisms of their operations
 and decision-making processes. Bridging this gap will require future research into integrating explainable AI (XAI)
 techniques within the LLMs themselves or adopting open-source models that provide full access to their architectures
 and reasoning pathways. This step would align the explainability of the entire system with the principles already
 embedded in the OVON-based agentic design.
 9 FutureImprovements
 To address the limitations, several future improvements can be proposed. One area of enhancement involves broadening
 the current agentic design itself. While the pipeline harnesses three core agents (plus a fourth for KPI evaluation), it
 is conceivable to add additional specialized agents that tackle specific tasks, such as fact-checking, cross-referencing
 domain-specific databases, or refining stylistic nuances. These extra layers of review could intensify the system’s
 capacity to detect subtle forms of hallucination and ensure an even higher degree of factual transparency.
 Moreover, future research would benefit from testing advanced large language models beyond the suite provided by
 OpenAI. Integrating models from various providers- such as Google’s Gemini or LaMDA-based systems, Anthropic’s
 Claude, or Meta’s Llama family, Mistral, and more- would reduce dependence on any single model architecture and
 thus mitigate vendor-specific biases. The resulting diversity of insights could enable a more robust triangulation of
 factual correctness or, at minimum, better highlight conflicting outputs among different models.
 Adesign that systematically orchestrates multiple agent levels across heterogeneous LLM architectures, supplemented
 by a richer human feedback loop, holds promise for further reducing hallucinations and boosting the overall reliability
 of generated content. Future work could enhance this approach by extending the use of OVON messages- such as
 utterance and whisper- across all agent interactions, ensuring a more structured and standardized exchange of contextual
 information at every stage of the pipeline.
 In addition to extending the use of core OVON messages, future implementations could leverage OVON extended
 events, such as the Discovery "findAssistant" event [1], which allows for the dynamic identification of AI agents with
 the most relevant expertise for a given user query. By enabling this dynamic agent discovery, the system could route
 specific prompts to specialized agents best equipped to handle the topic, ensuring more accurate and reliable responses.
 Furthermore, recent advancements in the field of Agentic AI have spurred the development of novel methodologies
 to further automate and enhance the design of agentic systems [13]. Notably, the introduction of the research area
 known as Automated Design of Agentic Systems (ADAS) marks a significant step forward. ADAS aims to automate
 the creation and configuration of AI agents by leveraging past discoveries, domain-specific data, and advanced machine
 16
A PREPRINT- JANUARY 27, 2025
 learning techniques. This approach has the potential to enhance the effectiveness, adaptability, and innovation of
 multi-agent frameworks.
 In the context of the OVON-based pipeline utilized in this study, integrating ADAS methodologies could further
 streamline the development and refinement of specialized agents. By dynamically generating agents tailored to specific
 tasks or domains, ADAS could enable systems to adapt to evolving requirements and optimize their performance in real
 time. This aligns with the study’s emphasis on modular, scalable, and transparent AI frameworks, underscoring the
 active and innovative nature of research in Agentic AI.
 Such capabilities not only would add an adaptive layer to the multi-agent pipeline but also holds potential for further
 mitigating hallucinations by reducing the likelihood of responses being generated by agents without sufficient domain
 expertise. Incorporating these advanced functionalities could significantly enhance the transparency, consistency, and
 effectiveness of multi-agent AI systems.
 10 Conclusion
 The multi-agent orchestration approach examined in this study suggests that using multiple, specialized agents can
 contribute to mitigating hallucinations in Large Language Models (LLMs). By pairing front-end creative generation with
 successive review stages that systematically insert disclaimers, reframe speculative statements, and reduce the density
 of supposed factual claims, the pipeline demonstrates a tangible decrease in hallucination scores. The incorporation of
 OVONJSONdialogevents, Natural Language-Based, particularly in the form of whisper context and whisper value
 f
 ields, further enables transparent data exchange among agents, facilitating targeted refinements without requiring each
 stage to restart the analysis process.
 Another contribution of this study is the introduction of novel Key Performance Indicators (KPIs) tailored to evaluate
 hallucination score levels. These metrics, including Factual Claim Density, Factual Grounding References, Fictional
 Disclaimer Frequency, and Explicit Contextualization Score, provide a structured framework for quantitatively assessing
 the reduction of hallucinations in multi-agent pipelines. By offering a measurable standard, these KPIs enhance the
 transparency of the evaluation process and support future research aimed at improving the reliability and interpretability
 of generative AI systems.
 Despite the high variance in hallucination mitigation scores between different prompts, the quantitative KPIs observed
 across three hundred and ten diverse prompts indicate that an iterative framework- one in which each agent builds on the
 output of the previous stage- can substantially modify the presentation of potential hallucinations. While these findings
 do not eliminate the fundamental challenges of LLM reliability, they highlight the benefits of combining architecture
 (multi-agent layering), structured NLP-based data transfer, and a modest level of human oversight. Additional manual
 checks at key junctures, as well as future experimentation with a broader set of LLM providers and more specialized
 agents, may yield further improvements in overall accuracy, clarity and Trustworthy AI.
 11 Acknowledgments
 We express our sincere appreciation to the Open Voice interoperability[14] Team (Linux Foundation AI & Data
 Foundation) for their invaluable contributions and support in developing the Interoperable Standards, particularly to
 Emmett Coin, David Attawater, Jon Stine, Jim Larson, Leah Barnes, Olga Howard, Noreen Whysel, and Allan Wylie.
 Their expertise, suggestions, and resources have been pivotal in shaping a model that is both ethically grounded and
 practically effective in real-world applications.
 References
