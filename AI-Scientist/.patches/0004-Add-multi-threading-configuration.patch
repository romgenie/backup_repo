diff --git a/templates/nanoGPT/experiment.py b/templates/nanoGPT/experiment.py
index 1cf2585..de5b807 100644
--- a/templates/nanoGPT/experiment.py
+++ b/templates/nanoGPT/experiment.py
@@ -318,7 +318,7 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     # -----------------------------------------------------------------------------
     # default config values designed to train a gpt2 (124M) on OpenWebText
     # data
-    gradient_accumulation_steps = 1
+    gradient_accumulation_steps = 4  # Increased from 1 to accumulate gradients over more batches
     batch_size = 64 if dataset == "shakespeare_char" else 32
     block_size = 256  # context of up to 256 previous characters
     # I/O
@@ -349,13 +349,30 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     # DDP settings
     backend = "nccl"  # 'nccl', 'gloo', etc.
     # system
-    device = "cuda"  # Always use CUDA
+    # Load device configuration from config_gpu.ini if it exists
+    try:
+        import configparser
+        import os
+        config = configparser.ConfigParser()
+        config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config_gpu.ini')
+        if os.path.exists(config_path):
+            config.read(config_path)
+            if 'gpu' in config and 'device' in config['gpu']:
+                device = config['gpu']['device'].strip('"').strip("'")
+            else:
+                device = "cuda"  # Default to CUDA if config exists but doesn't specify device
+        else:
+            device = "cuda"  # Default to CUDA if no config file
+    except:
+        # Fall back to default if any error occurs with config
+        device = "cuda"    # Always use CUDA
     dtype = (
         "bfloat16"
         if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
         else "float16"
     )  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
-    compile = True  # do not torch compile the model on macbooks
+    #compile = True  # do not torch compile the model on macbooks
+    compile = False  # disabled torch compile for macbooks with MPS
 
     # various inits, derived attributes, I/O setup
     # if not ddp, we are running on a single gpu, and one process
@@ -366,6 +383,10 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     if master_process:
         os.makedirs(out_dir, exist_ok=True)
     torch.manual_seed(1337 + seed_offset)
+    
+    # Set number of threads for CPU operations
+    torch.set_num_threads(8)  # Use 8 threads for CPU operations
+    
     torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul
     torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn
     device_type = (
