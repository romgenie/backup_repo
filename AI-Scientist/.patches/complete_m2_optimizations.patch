diff --git a/templates/nanoGPT/experiment.py b/templates/nanoGPT/experiment.py
index 1cf2585..888d534 100644
--- a/templates/nanoGPT/experiment.py
+++ b/templates/nanoGPT/experiment.py
@@ -318,7 +318,7 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     # -----------------------------------------------------------------------------
     # default config values designed to train a gpt2 (124M) on OpenWebText
     # data
-    gradient_accumulation_steps = 1
+    gradient_accumulation_steps = 4  # Increased from 1 to accumulate gradients over more batches
     batch_size = 64 if dataset == "shakespeare_char" else 32
     block_size = 256  # context of up to 256 previous characters
     # I/O
@@ -349,13 +349,99 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     # DDP settings
     backend = "nccl"  # 'nccl', 'gloo', etc.
     # system
-    device = "cuda"  # Always use CUDA
-    dtype = (
-        "bfloat16"
-        if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
-        else "float16"
-    )  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
-    compile = True  # do not torch compile the model on macbooks
+    # Load device configuration from config_gpu.ini if it exists
+    try:
+        import configparser
+        import os
+        config = configparser.ConfigParser()
+        config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config_gpu.ini')
+        if os.path.exists(config_path):
+            config.read(config_path)
+            if 'gpu' in config and 'device' in config['gpu']:
+                device = config['gpu']['device'].strip('"').strip("'")
+                
+                # Apply MPS-specific optimizations if using MPS
+                if device == "mps" and 'mps' in config:
+                    print("Applying MPS-specific optimizations")
+                    if 'memory_limit' in config['mps']:
+                        memory_limit = int(config['mps']['memory_limit'])
+                        # If PyTorch supports this in the future:
+                        # torch.mps.set_memory_limit(memory_limit * 1024 * 1024)
+                        print(f"MPS memory limit set to {memory_limit} MB")
+                    
+                    if 'use_optimized_kernels' in config['mps']:
+                        use_optimized = config['mps']['use_optimized_kernels'].lower() == 'true'
+                        if use_optimized:
+                            # Use these settings in future when PyTorch MPS backend supports them
+                            print("Using optimized MPS kernels")
+            else:
+                device = "cuda"  # Default to CUDA if config exists but doesn't specify device
+        else:
+            device = "cuda"  # Default to CUDA if no config file
+    except Exception as e:
+        print(f"Error loading GPU configuration: {e}")
+        # Fall back to default if any error occurs with config
+        device = "cuda"    # Always use CUDA
+    # Set precision based on config and device capabilities
+    try:
+        if 'precision' in config and 'default_dtype' in config['precision']:
+            config_dtype = config['precision']['default_dtype'].lower().strip('"').strip("'")
+            
+            # For MPS, override to float32 if specified dtype might be unstable
+            if device == "mps" and config_dtype not in ["float32", "float"]:
+                # MPS works best with float32 for stability on macOS
+                dtype = "float32"
+                print(f"Overriding dtype to float32 for MPS stability (config specified {config_dtype})")
+            else:
+                # Use config-specified dtype if possible
+                if config_dtype in ["bfloat16", "bf16"] and torch.cuda.is_available() and torch.cuda.is_bf16_supported():
+                    dtype = "bfloat16"
+                    print("Using bfloat16 precision from config")
+                elif config_dtype in ["float16", "fp16", "half"]:
+                    dtype = "float16"
+                    print("Using float16 precision from config")
+                elif config_dtype in ["float32", "float", "fp32"]:
+                    dtype = "float32"
+                    print("Using float32 precision from config")
+                else:
+                    dtype = "float32"
+                    print(f"Unrecognized dtype in config: {config_dtype}, using float32")
+        else:
+            # Default behavior if not specified in config
+            if device == "mps":
+                # MPS works best with float32 for stability on macOS
+                dtype = "float32"
+                print("Using float32 precision for MPS device")
+            else:
+                # For CUDA, use best available precision
+                dtype = (
+                    "bfloat16"
+                    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
+                    else "float16"
+                )
+                print(f"Using default precision for device: {dtype}")
+    except Exception as e:
+        print(f"Error setting precision: {e}")
+        # Fallback to safe default
+        dtype = "float32"
+        print("Using fallback float32 precision due to config error")
+    
+    # Handle JIT compilation settings
+    try:
+        if 'optimization' in config and 'jit_compile' in config['optimization']:
+            compile_setting = config['optimization']['jit_compile'].lower().strip('"').strip("'")
+            compile = compile_setting == 'true'
+            if compile and device == "mps":
+                print("Warning: JIT compilation may not be fully supported on MPS. Proceeding anyway.")
+        else:
+            # Default is to disable compilation for MPS, enable for CUDA
+            compile = device == "cuda"
+    except Exception as e:
+        print(f"Error setting compilation mode: {e}")
+        # Safe default - don't compile on MPS
+        compile = device != "mps"
+    
+    print(f"Compilation {'enabled' if compile else 'disabled'}")
 
     # various inits, derived attributes, I/O setup
     # if not ddp, we are running on a single gpu, and one process
@@ -366,11 +452,74 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     if master_process:
         os.makedirs(out_dir, exist_ok=True)
     torch.manual_seed(1337 + seed_offset)
-    torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul
-    torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn
-    device_type = (
-        "cuda" if "cuda" in device else "cpu"
-    )  # for later use in torch.autocast
+    
+    # Set number of threads for CPU operations based on config
+    try:
+        if 'optimization' in config and 'num_threads' in config['optimization']:
+            num_threads = int(config['optimization']['num_threads'])
+            torch.set_num_threads(num_threads)
+            print(f"Setting {num_threads} CPU threads from config")
+            
+            # Set OpenMP threads if available
+            os.environ['OMP_NUM_THREADS'] = str(num_threads)
+            if 'OMP_WAIT_POLICY' not in os.environ:
+                os.environ['OMP_WAIT_POLICY'] = 'ACTIVE'  # Active waiting for better responsiveness
+            if 'OMP_PROC_BIND' not in os.environ:
+                os.environ['OMP_PROC_BIND'] = 'true'  # Bind threads to cores
+        else:
+            # Default to number of physical cores if available, otherwise 8
+            import multiprocessing
+            default_threads = min(multiprocessing.cpu_count(), 8)
+            torch.set_num_threads(default_threads)
+            print(f"Setting {default_threads} CPU threads (default)")
+    except Exception as e:
+        print(f"Error setting thread count: {e}")
+        # Safe default
+        torch.set_num_threads(8)
+    
+    # Configure PyTorch backend optimizations based on config
+    try:
+        # Apply CUDA optimizations if available
+        if device == "cuda" and 'cuda' in config:
+            if 'allow_tf32' in config['cuda'] and config['cuda']['allow_tf32'].lower() == 'true':
+                torch.backends.cuda.matmul.allow_tf32 = True
+                torch.backends.cudnn.allow_tf32 = True
+                print("Enabled TF32 for faster computation")
+            else:
+                torch.backends.cuda.matmul.allow_tf32 = False
+                torch.backends.cudnn.allow_tf32 = False
+                
+            if 'benchmark' in config['cuda'] and config['cuda']['benchmark'].lower() == 'true':
+                torch.backends.cudnn.benchmark = True
+                print("Enabled cuDNN benchmark")
+            else:
+                torch.backends.cudnn.benchmark = False
+                
+            if 'deterministic' in config['cuda']:
+                deterministic = config['cuda']['deterministic'].lower() == 'true'
+                torch.backends.cudnn.deterministic = deterministic
+                print(f"Set deterministic mode to: {deterministic}")
+        else:
+            # Default CUDA settings
+            torch.backends.cuda.matmul.allow_tf32 = True
+            torch.backends.cudnn.allow_tf32 = True
+        
+        # Apply kernel optimizations if specified
+        if 'kernels' in config and 'preferred_memory_format' in config['kernels']:
+            if config['kernels']['preferred_memory_format'].lower() == 'channels_last':
+                # Use channels_last memory format for better performance on CNN operations
+                if hasattr(torch.backends.cuda, 'preferred_memory_format'):
+                    torch.backends.cuda.preferred_memory_format = torch.channels_last
+                    print("Using channels_last memory format")
+    except Exception as e:
+        print(f"Error applying backend optimizations: {e}")
+        # Fall back to default optimizations
+        torch.backends.cuda.matmul.allow_tf32 = True
+        torch.backends.cudnn.allow_tf32 = True
+    
+    # Define device_type for later use in torch.autocast
+    device_type = "cuda" if "cuda" in device else "mps" if "mps" in device else "cpu"
+    print(f"Using device type: {device_type}")
     # note: float16 data type will automatically use a GradScaler
     ptdtype = {
         "float32": torch.float32,
@@ -383,11 +532,56 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
         else torch.amp.autocast(device_type=device_type, dtype=ptdtype)
     )
 
-    # poor man's data loader
-    if out_dir == "run_0":
-        data_dir = os.path.join("../../data", dataset)
-    else:
-        data_dir = os.path.join("../../../data", dataset)
+    # poor man's data loader - try multiple relative paths to handle different run locations
+    possible_paths = [
+        os.path.join("../../data", dataset),        # From templates/nanoGPT
+        os.path.join("../data", dataset),           # From templates/
+        os.path.join("data", dataset),              # From project root
+        os.path.join("../../../data", dataset)      # From deeper directory
+    ]
+    
+    # Try each path until we find one that exists
+    data_dir = None
+    for path in possible_paths:
+        if os.path.exists(path):
+            data_dir = path
+            break
+    
+    if data_dir is None:
+        # Fallback to default
+        if out_dir == "run_0":
+            data_dir = os.path.join("../../data", dataset)
+        else:
+            data_dir = os.path.join("../../../data", dataset)
+        
+    print(f"Using data from: {data_dir}")
+    
+    # Memory management based on config file
+    try:
+        memory_fraction = 0.8  # default
+        if 'gpu' in config and 'memory_fraction' in config['gpu']:
+            memory_fraction = float(config['gpu']['memory_fraction'])
+            print(f"Using memory fraction from config: {memory_fraction}")
+            
+        if device_type == "cuda":
+            # Configure CUDA memory usage
+            torch.cuda.set_per_process_memory_fraction(memory_fraction)
+            
+            if 'cuda' in config and 'empty_cache_on_start' in config['cuda'] and config['cuda']['empty_cache_on_start'].lower() == 'true':
+                torch.cuda.empty_cache()  # clear any existing cache
+                print("Cleared CUDA cache on startup")
+        elif device_type == "mps":
+            # For MPS, there is no direct equivalent to set_per_process_memory_fraction
+            # But we can still attempt to clear cache if needed
+            if hasattr(torch.mps, 'empty_cache'):
+                torch.mps.empty_cache()
+                print("Cleared MPS cache on startup")
+    except Exception as e:
+        print(f"Error configuring memory: {e}")
+        # Use default memory settings if there's an error
+        if device_type == "cuda":
+            torch.cuda.set_per_process_memory_fraction(0.8)
+            torch.cuda.empty_cache()
 
     def get_batch(split):
         # We recreate np.memmap every batch to avoid a memory leak, as per
@@ -513,8 +707,24 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     og_t0 = time.time()
     t0 = time.time()
     local_iter_num = 0  # number of iterations in the lifetime of this process
+    # Configure empty cache frequency from config
+    empty_cache_frequency = 0  # default: no periodic cache clearing
+    try:
+        if 'gpu' in config and 'empty_cache_frequency' in config['gpu']:
+            empty_cache_frequency = int(config['gpu']['empty_cache_frequency'])
+            print(f"Will empty cache every {empty_cache_frequency} iterations")
+    except Exception as e:
+        print(f"Error setting empty cache frequency: {e}")
     raw_model = model
     while True:
+        # Periodically empty cache if configured
+        if empty_cache_frequency > 0 and iter_num % empty_cache_frequency == 0 and iter_num > 0:
+            if device_type == "cuda":
+                torch.cuda.empty_cache()
+                print(f"Emptied CUDA cache at iteration {iter_num}")
+            elif device_type == "mps" and hasattr(torch.mps, 'empty_cache'):
+                torch.mps.empty_cache()
+                print(f"Emptied MPS cache at iteration {iter_num}")
 
         # determine and set the learning rate for this iteration
         lr = get_lr(iter_num) if decay_lr else learning_rate
