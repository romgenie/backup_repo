diff --git a/templates/nanoGPT/experiment.py b/templates/nanoGPT/experiment.py
index 1cf2585..c1cc24b 100644
--- a/templates/nanoGPT/experiment.py
+++ b/templates/nanoGPT/experiment.py
@@ -318,7 +318,7 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     # -----------------------------------------------------------------------------
     # default config values designed to train a gpt2 (124M) on OpenWebText
     # data
-    gradient_accumulation_steps = 1
+    gradient_accumulation_steps = 4  # Increased from 1 to accumulate gradients over more batches
     batch_size = 64 if dataset == "shakespeare_char" else 32
     block_size = 256  # context of up to 256 previous characters
     # I/O
@@ -349,13 +349,53 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     # DDP settings
     backend = "nccl"  # 'nccl', 'gloo', etc.
     # system
-    device = "cuda"  # Always use CUDA
-    dtype = (
-        "bfloat16"
-        if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
-        else "float16"
-    )  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
-    compile = True  # do not torch compile the model on macbooks
+    # Load device configuration from config_gpu.ini if it exists
+    try:
+        import configparser
+        import os
+        config = configparser.ConfigParser()
+        config_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config_gpu.ini')
+        if os.path.exists(config_path):
+            config.read(config_path)
+            if 'gpu' in config and 'device' in config['gpu']:
+                device = config['gpu']['device'].strip('"').strip("'")
+                
+                # Apply MPS-specific optimizations if using MPS
+                if device == "mps" and 'mps' in config:
+                    print("Applying MPS-specific optimizations")
+                    if 'memory_limit' in config['mps']:
+                        memory_limit = int(config['mps']['memory_limit'])
+                        # If PyTorch supports this in the future:
+                        # torch.mps.set_memory_limit(memory_limit * 1024 * 1024)
+                        print(f"MPS memory limit set to {memory_limit} MB")
+                    
+                    if 'use_optimized_kernels' in config['mps']:
+                        use_optimized = config['mps']['use_optimized_kernels'].lower() == 'true'
+                        if use_optimized:
+                            # Use these settings in future when PyTorch MPS backend supports them
+                            print("Using optimized MPS kernels")
+            else:
+                device = "cuda"  # Default to CUDA if config exists but doesn't specify device
+        else:
+            device = "cuda"  # Default to CUDA if no config file
+    except Exception as e:
+        print(f"Error loading GPU configuration: {e}")
+        # Fall back to default if any error occurs with config
+        device = "cuda"    # Always use CUDA
+    # Set optimal dtype based on device
+    if device == "mps":
+        # MPS works best with float32 for stability on macOS
+        dtype = "float32"
+        print("Using float32 precision for MPS device")
+    else:
+        # For CUDA, use best available precision
+        dtype = (
+            "bfloat16"
+            if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
+            else "float16"
+        )  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
+    #compile = True  # do not torch compile the model on macbooks
+    compile = False  # disabled torch compile for macbooks with MPS
 
     # various inits, derived attributes, I/O setup
     # if not ddp, we are running on a single gpu, and one process
@@ -366,11 +406,24 @@ def train(dataset="shakespeare_char", out_dir="run_0", seed_offset=0):
     if master_process:
         os.makedirs(out_dir, exist_ok=True)
     torch.manual_seed(1337 + seed_offset)
+    
+    # Set number of threads for CPU operations
+    torch.set_num_threads(8)  # Use 8 threads for CPU operations
+    
+    # PyTorch backend optimizations
     torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul
     torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn
-    device_type = (
-        "cuda" if "cuda" in device else "cpu"
-    )  # for later use in torch.autocast
+    torch.backends.cudnn.benchmark = True  # enable cudnn auto-tuner
+    torch.backends.cudnn.deterministic = False  # more efficient when not needed
+    
+    # Define device_type for later use
+    device_type = "cuda" if "cuda" in device else "mps" if "mps" in device else "cpu"
+    
+    # Memory management
+    if device_type == "cuda":
+        # Reserved memory size - minimum amount CUDA will allocate
+        torch.cuda.set_per_process_memory_fraction(0.8)  # use up to 80% of available memory
+        torch.cuda.empty_cache()  # clear any existing cache
     # note: float16 data type will automatically use a GradScaler
     ptdtype = {
         "float32": torch.float32,
